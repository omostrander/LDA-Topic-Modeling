{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad388e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/omostrander/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/omostrander/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#Gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beefd6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/.DS_Store\n",
      "./data/abcnews-date-text.csv\n"
     ]
    }
   ],
   "source": [
    "#print items in working directory\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ee7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset from the csv and save it to 'data_text'\n",
    "data = pd.read_csv('./data/abcnews-date-text.csv')\n",
    "\n",
    "# we only need to headlines from the data\n",
    "data_text = data[:300000][['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc7d988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the total number of documents\n",
    "print(len(documents), \"\\n\")\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97f5ad",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "- Tokenization\n",
    "  - Sentences -> words\n",
    "  - Lowercase all words\n",
    "  - Remove punctuation \n",
    "- Remove stopwords\n",
    "- Lemmatization\n",
    "  - 3rd person -> to 1st person\n",
    "  - Verbs: past and future tenses converted to present tense\n",
    "- Stem words - reducing them to their root forms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc2c1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to perform the pre-processing steps on the entire dataset\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and Lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9bdf3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n",
      "\n",
      "                                        headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "#preview document after pre-processing\n",
    "\n",
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))\n",
    "print(\"\\n\", documents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affca94",
   "metadata": {},
   "source": [
    "Let's now preprocess all the news headlines we have. To do that, let's use the map function from pandas to apply preprocess() to the headline_text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc700a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [decid, communiti, broadcast, licenc]\n",
       "1                        [wit, awar, defam]\n",
       "2    [call, infrastructur, protect, summit]\n",
       "3               [staff, aust, strike, rise]\n",
       "4      [strike, affect, australian, travel]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess all the headlines, saving the list of results as 'processed_docs'\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "\n",
    "#preview processed docs\n",
    "processed_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a859e2",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset\n",
    "Now let's create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's pass processed_docs to gensim.corpora.Dictionary() and call it 'dictionary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67049778",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912180ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "#Verify the dictionary creation\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584cd4e3",
   "metadata": {},
   "source": [
    "### Gensim filter_extremes \n",
    "- filter_extremes(no_below=5, no_above=0.5, keep_n=100000) \n",
    "- Filter out tokens that appear in \n",
    "- less than no_below documents (absolute number) or\n",
    "  more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "  after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
    "\n",
    "In the example below, we will remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c9f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove very rare and very common words\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7c0f9",
   "metadata": {},
   "source": [
    "#### Gensim doc2bow\n",
    "\n",
    "doc2bow(document)\n",
    "\n",
    "- Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further    preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdc80f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert document into the bag-of-words format\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21c362b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(154, 1), (228, 1), (276, 1), (563, 1), (806, 1), (3175, 1), (3176, 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Bag-of-Words corpus for our sample document --> (token_id, token_count)\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d809b",
   "metadata": {},
   "source": [
    "### Preview BOW for our sample preprocessed document\n",
    "\n",
    "Here document_num is document number 4310 which we have checked in Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bd04866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 154 (\"govt\") appears 1 time.\n",
      "Word 228 (\"group\") appears 1 time.\n",
      "Word 276 (\"vote\") appears 1 time.\n",
      "Word 563 (\"local\") appears 1 time.\n",
      "Word 806 (\"want\") appears 1 time.\n",
      "Word 3175 (\"compulsori\") appears 1 time.\n",
      "Word 3176 (\"ratepay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#Preview BOW for our sample pre-processed document\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c520624",
   "metadata": {},
   "source": [
    "#### TF-IDF on our document set\n",
    "While performing TF-IDF on the corpus is not necessary for LDA implemention using the gensim model, it is recemmended. TF-IDF expects a bag-of-words (integer values) training corpus during initialization. During transformation, it will take a vector and return another vector of the same dimensionality.\n",
    "\n",
    "- TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "- IDF(w) = log_e(Total number of documents / Number of documents with term w in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "785563a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959919082495837),\n",
      " (1, 0.3920069955308767),\n",
      " (2, 0.48532280284497653),\n",
      " (3, 0.5055550788930631)]\n"
     ]
    }
   ],
   "source": [
    "#Create tf-idf model object using models.TfidfModel on 'bow_corpus' and save it to 'tfidf'\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "#Apply transformation to the entire corpus and call it 'corpus_tfidf'\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "#Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86b540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
