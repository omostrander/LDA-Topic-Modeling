{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5237a8d8",
   "metadata": {},
   "source": [
    "This notebook and data are derived from a Kaggle tutorial:\n",
    "\n",
    "https://www.kaggle.com/code/nilaychauhan/topic-modeling-of-news-articles-lda/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad388e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/omostrander/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/omostrander/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#Gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beefd6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/.DS_Store\n",
      "./data/abcnews-date-text.csv\n"
     ]
    }
   ],
   "source": [
    "#print items in working directory\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ee7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset from the csv and save it to 'data_text'\n",
    "data = pd.read_csv('./data/abcnews-date-text.csv')\n",
    "\n",
    "# we only need to headlines from the data\n",
    "data_text = data[:300000][['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a1bbca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the total number of documents\n",
    "print(len(documents), \"\\n\")\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fccbf",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "- Tokenization\n",
    "  - Sentences -> words\n",
    "  - Lowercase all words\n",
    "  - Remove punctuation \n",
    "- Remove stopwords\n",
    "- Lemmatization\n",
    "  - 3rd person -> to 1st person\n",
    "  - Verbs: past and future tenses converted to present tense\n",
    "- Stem words - reducing them to their root forms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a46cc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to perform the pre-processing steps on the entire dataset\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and Lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d71c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n",
      "\n",
      "                                        headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "#preview document after pre-processing\n",
    "\n",
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))\n",
    "print(\"\\n\", documents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c77e2",
   "metadata": {},
   "source": [
    "Let's now preprocess all the news headlines we have. To do that, let's use the map function from pandas to apply preprocess() to the headline_text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3453ddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [decid, communiti, broadcast, licenc]\n",
       "1                        [wit, awar, defam]\n",
       "2    [call, infrastructur, protect, summit]\n",
       "3               [staff, aust, strike, rise]\n",
       "4      [strike, affect, australian, travel]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess all the headlines, saving the list of results as 'processed_docs'\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "\n",
    "#preview processed docs\n",
    "processed_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4927a",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset\n",
    "Now let's create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's pass processed_docs to gensim.corpora.Dictionary() and call it 'dictionary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8e0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e322b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "#Verify the dictionary creation\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db0bff",
   "metadata": {},
   "source": [
    "### Gensim filter_extremes \n",
    "- filter_extremes(no_below=5, no_above=0.5, keep_n=100000) \n",
    "- Filter out tokens that appear in \n",
    "- less than no_below documents (absolute number) or\n",
    "  more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "  after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
    "\n",
    "In the example below, we will remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c610ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove very rare and very common words\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65dd621",
   "metadata": {},
   "source": [
    "#### Gensim doc2bow\n",
    "\n",
    "doc2bow(document)\n",
    "\n",
    "- Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further    preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66fcab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert document into the bag-of-words format\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08b0071c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(154, 1), (228, 1), (276, 1), (563, 1), (806, 1), (3175, 1), (3176, 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Bag-of-Words corpus for our sample document --> (token_id, token_count)\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e619052",
   "metadata": {},
   "source": [
    "### Preview BOW for our sample preprocessed document\n",
    "\n",
    "Here document_num is document number 4310 which we have checked in Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b4a746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 154 (\"govt\") appears 1 time.\n",
      "Word 228 (\"group\") appears 1 time.\n",
      "Word 276 (\"vote\") appears 1 time.\n",
      "Word 563 (\"local\") appears 1 time.\n",
      "Word 806 (\"want\") appears 1 time.\n",
      "Word 3175 (\"compulsori\") appears 1 time.\n",
      "Word 3176 (\"ratepay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#Preview BOW for our sample pre-processed document\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06342ac4",
   "metadata": {},
   "source": [
    "#### TF-IDF on our document set\n",
    "While performing TF-IDF on the corpus is not necessary for LDA implemention using the gensim model, it is recemmended. TF-IDF expects a bag-of-words (integer values) training corpus during initialization. During transformation, it will take a vector and return another vector of the same dimensionality.\n",
    "\n",
    "- TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "- IDF(w) = log_e(Total number of documents / Number of documents with term w in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e05d9373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959919082495837),\n",
      " (1, 0.3920069955308767),\n",
      " (2, 0.48532280284497653),\n",
      " (3, 0.5055550788930631)]\n"
     ]
    }
   ],
   "source": [
    "#Create tf-idf model object using models.TfidfModel on 'bow_corpus' and save it to 'tfidf'\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "#Apply transformation to the entire corpus and call it 'corpus_tfidf'\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "#Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbeca3",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words\n",
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "We will be running LDA using all CPU cores to parallelize and speed up model training.\n",
    "\n",
    "Some of the parameters we will be tweaking are:\n",
    "\n",
    "- number of topics\n",
    "- id2word (determine vocabulary size)\n",
    "- workers\n",
    "- alpha (per document, topic distribution: higher they are more similar, less and the documents are less similar)\n",
    "- eta (per word distribution)\n",
    "- passes (number of training passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf62c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "\n",
    "#Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=10, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e852e115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0.023*\"open\" + 0.019*\"win\" + 0.015*\"time\" + 0.015*\"leav\" + 0.012*\"australian\" + 0.012*\"rescu\" + 0.011*\"award\" + 0.011*\"nuclear\" + 0.011*\"back\" + 0.011*\"damag\" \n",
      "Words: 0\n",
      "\n",
      "\n",
      "Topic: 0.036*\"charg\" + 0.031*\"polic\" + 0.022*\"miss\" + 0.019*\"murder\" + 0.017*\"attack\" + 0.015*\"woman\" + 0.014*\"search\" + 0.014*\"arrest\" + 0.014*\"home\" + 0.014*\"drug\" \n",
      "Words: 1\n",
      "\n",
      "\n",
      "Topic: 0.032*\"report\" + 0.026*\"iraq\" + 0.021*\"forc\" + 0.017*\"kill\" + 0.016*\"test\" + 0.013*\"leader\" + 0.013*\"troop\" + 0.012*\"releas\" + 0.010*\"work\" + 0.010*\"hick\" \n",
      "Words: 2\n",
      "\n",
      "\n",
      "Topic: 0.024*\"urg\" + 0.021*\"health\" + 0.019*\"servic\" + 0.018*\"worker\" + 0.017*\"govt\" + 0.016*\"help\" + 0.015*\"opposit\" + 0.013*\"jail\" + 0.013*\"communiti\" + 0.013*\"say\" \n",
      "Words: 3\n",
      "\n",
      "\n",
      "Topic: 0.062*\"polic\" + 0.036*\"crash\" + 0.032*\"death\" + 0.026*\"investig\" + 0.020*\"road\" + 0.016*\"probe\" + 0.015*\"year\" + 0.015*\"break\" + 0.013*\"driver\" + 0.010*\"fatal\" \n",
      "Words: 4\n",
      "\n",
      "\n",
      "Topic: 0.046*\"plan\" + 0.034*\"council\" + 0.019*\"govt\" + 0.019*\"water\" + 0.015*\"group\" + 0.014*\"chang\" + 0.014*\"closer\" + 0.013*\"nation\" + 0.010*\"urg\" + 0.009*\"fund\" \n",
      "Words: 5\n",
      "\n",
      "\n",
      "Topic: 0.015*\"south\" + 0.015*\"rain\" + 0.014*\"australia\" + 0.012*\"guilti\" + 0.012*\"storm\" + 0.011*\"flood\" + 0.011*\"england\" + 0.011*\"east\" + 0.010*\"cyclon\" + 0.009*\"world\" \n",
      "Words: 6\n",
      "\n",
      "\n",
      "Topic: 0.040*\"govt\" + 0.019*\"hospit\" + 0.019*\"reject\" + 0.018*\"court\" + 0.018*\"face\" + 0.017*\"power\" + 0.014*\"industri\" + 0.014*\"fund\" + 0.013*\"accus\" + 0.013*\"claim\" \n",
      "Words: 7\n",
      "\n",
      "\n",
      "Topic: 0.040*\"warn\" + 0.018*\"die\" + 0.017*\"deal\" + 0.017*\"coast\" + 0.016*\"north\" + 0.015*\"talk\" + 0.014*\"price\" + 0.013*\"bodi\" + 0.012*\"gold\" + 0.012*\"threat\" \n",
      "Words: 8\n",
      "\n",
      "\n",
      "Topic: 0.024*\"hous\" + 0.017*\"high\" + 0.016*\"market\" + 0.014*\"busi\" + 0.014*\"record\" + 0.013*\"sale\" + 0.012*\"aussi\" + 0.010*\"share\" + 0.010*\"say\" + 0.010*\"iran\" \n",
      "Words: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(topic, idx ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222aa222",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23d2bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lda model using corpus_tfidf, again using gensim.models.LdaMulticore()\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                       num_topics=10, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc621952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.010*\"hick\" + 0.009*\"price\" + 0.009*\"liber\" + 0.008*\"govt\" + 0.008*\"drink\" + 0.008*\"sale\" + 0.007*\"drive\" + 0.007*\"say\" + 0.007*\"water\" + 0.006*\"iemma\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.008*\"grower\" + 0.008*\"toll\" + 0.008*\"export\" + 0.007*\"miner\" + 0.006*\"week\" + 0.006*\"coal\" + 0.006*\"theft\" + 0.006*\"afghanistan\" + 0.006*\"revamp\" + 0.006*\"violenc\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.023*\"crash\" + 0.021*\"kill\" + 0.016*\"polic\" + 0.014*\"miss\" + 0.012*\"search\" + 0.011*\"die\" + 0.010*\"iraq\" + 0.010*\"investig\" + 0.009*\"attack\" + 0.009*\"dead\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.046*\"closer\" + 0.008*\"fish\" + 0.007*\"scientist\" + 0.007*\"cancer\" + 0.006*\"lake\" + 0.006*\"central\" + 0.006*\"doubt\" + 0.006*\"illeg\" + 0.005*\"monitor\" + 0.005*\"confirm\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.010*\"market\" + 0.007*\"solomon\" + 0.007*\"eas\" + 0.007*\"hill\" + 0.007*\"takeov\" + 0.006*\"break\" + 0.006*\"rate\" + 0.006*\"rat\" + 0.006*\"council\" + 0.006*\"rise\"\n",
      "\n",
      "\n",
      "Topic: 5 Word: 0.010*\"councillor\" + 0.009*\"indonesia\" + 0.008*\"speed\" + 0.008*\"steal\" + 0.008*\"retir\" + 0.007*\"chase\" + 0.007*\"polic\" + 0.007*\"polit\" + 0.006*\"australia\" + 0.006*\"england\"\n",
      "\n",
      "\n",
      "Topic: 6 Word: 0.021*\"charg\" + 0.020*\"court\" + 0.016*\"murder\" + 0.013*\"face\" + 0.013*\"jail\" + 0.013*\"blaze\" + 0.012*\"polic\" + 0.011*\"assault\" + 0.011*\"firefight\" + 0.010*\"accus\"\n",
      "\n",
      "\n",
      "Topic: 7 Word: 0.011*\"govt\" + 0.010*\"health\" + 0.009*\"servic\" + 0.008*\"opposit\" + 0.008*\"indigen\" + 0.007*\"urg\" + 0.007*\"plan\" + 0.007*\"hospit\" + 0.006*\"union\" + 0.006*\"fund\"\n",
      "\n",
      "\n",
      "Topic: 8 Word: 0.010*\"timor\" + 0.010*\"guilti\" + 0.009*\"nuclear\" + 0.009*\"east\" + 0.007*\"iran\" + 0.007*\"plead\" + 0.006*\"uranium\" + 0.005*\"season\" + 0.005*\"agre\" + 0.005*\"injuri\"\n",
      "\n",
      "\n",
      "Topic: 9 Word: 0.018*\"water\" + 0.012*\"drought\" + 0.010*\"plan\" + 0.010*\"govt\" + 0.009*\"council\" + 0.008*\"rain\" + 0.008*\"farm\" + 0.008*\"farmer\" + 0.007*\"fund\" + 0.006*\"urg\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1497e",
   "metadata": {},
   "source": [
    "###  Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "\n",
    "We will check to see where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fd5fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n",
      "\n",
      "Score: 0.6282708048820496\t \n",
      "Topic: 0.046*\"plan\" + 0.034*\"council\" + 0.019*\"govt\" + 0.019*\"water\" + 0.015*\"group\" + 0.014*\"chang\" + 0.014*\"closer\" + 0.013*\"nation\" + 0.010*\"urg\" + 0.009*\"fund\"\n",
      "\n",
      "Score: 0.14638423919677734\t \n",
      "Topic: 0.015*\"south\" + 0.015*\"rain\" + 0.014*\"australia\" + 0.012*\"guilti\" + 0.012*\"storm\" + 0.011*\"flood\" + 0.011*\"england\" + 0.011*\"east\" + 0.010*\"cyclon\" + 0.009*\"world\"\n",
      "\n",
      "Score: 0.13780218362808228\t \n",
      "Topic: 0.062*\"polic\" + 0.036*\"crash\" + 0.032*\"death\" + 0.026*\"investig\" + 0.020*\"road\" + 0.016*\"probe\" + 0.015*\"year\" + 0.015*\"break\" + 0.013*\"driver\" + 0.010*\"fatal\"\n",
      "\n",
      "Score: 0.012507736682891846\t \n",
      "Topic: 0.040*\"govt\" + 0.019*\"hospit\" + 0.019*\"reject\" + 0.018*\"court\" + 0.018*\"face\" + 0.017*\"power\" + 0.014*\"industri\" + 0.014*\"fund\" + 0.013*\"accus\" + 0.013*\"claim\"\n",
      "\n",
      "Score: 0.01250767707824707\t \n",
      "Topic: 0.024*\"urg\" + 0.021*\"health\" + 0.019*\"servic\" + 0.018*\"worker\" + 0.017*\"govt\" + 0.016*\"help\" + 0.015*\"opposit\" + 0.013*\"jail\" + 0.013*\"communiti\" + 0.013*\"say\"\n",
      "\n",
      "Score: 0.01250557042658329\t \n",
      "Topic: 0.040*\"warn\" + 0.018*\"die\" + 0.017*\"deal\" + 0.017*\"coast\" + 0.016*\"north\" + 0.015*\"talk\" + 0.014*\"price\" + 0.013*\"bodi\" + 0.012*\"gold\" + 0.012*\"threat\"\n",
      "\n",
      "Score: 0.012505462393164635\t \n",
      "Topic: 0.023*\"open\" + 0.019*\"win\" + 0.015*\"time\" + 0.015*\"leav\" + 0.012*\"australian\" + 0.012*\"rescu\" + 0.011*\"award\" + 0.011*\"nuclear\" + 0.011*\"back\" + 0.011*\"damag\"\n",
      "\n",
      "Score: 0.012505442835390568\t \n",
      "Topic: 0.024*\"hous\" + 0.017*\"high\" + 0.016*\"market\" + 0.014*\"busi\" + 0.014*\"record\" + 0.013*\"sale\" + 0.012*\"aussi\" + 0.010*\"share\" + 0.010*\"say\" + 0.010*\"iran\"\n",
      "\n",
      "Score: 0.012505440972745419\t \n",
      "Topic: 0.036*\"charg\" + 0.031*\"polic\" + 0.022*\"miss\" + 0.019*\"murder\" + 0.017*\"attack\" + 0.015*\"woman\" + 0.014*\"search\" + 0.014*\"arrest\" + 0.014*\"home\" + 0.014*\"drug\"\n",
      "\n",
      "Score: 0.012505440972745419\t \n",
      "Topic: 0.032*\"report\" + 0.026*\"iraq\" + 0.021*\"forc\" + 0.017*\"kill\" + 0.016*\"test\" + 0.013*\"leader\" + 0.013*\"troop\" + 0.012*\"releas\" + 0.010*\"work\" + 0.010*\"hick\"\n"
     ]
    }
   ],
   "source": [
    "#Text of sample document 4310\n",
    "print(processed_docs[4310])\n",
    "\n",
    "#Check which topic our test document belongs to using the LDA Bag of Words model.\n",
    "document_num = 4310\n",
    "\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190b746",
   "metadata": {},
   "source": [
    "It has the highest probability (0.61) to be part of the topic that we assigned as Topic X, which is the accurate classification.\n",
    "\n",
    "---\n",
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20c1eb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.48170599341392517\t \n",
      "Topic: 0.018*\"water\" + 0.012*\"drought\" + 0.010*\"plan\" + 0.010*\"govt\" + 0.009*\"council\" + 0.008*\"rain\" + 0.008*\"farm\" + 0.008*\"farmer\" + 0.007*\"fund\" + 0.006*\"urg\"\n",
      "\n",
      "Score: 0.25961044430732727\t \n",
      "Topic: 0.046*\"closer\" + 0.008*\"fish\" + 0.007*\"scientist\" + 0.007*\"cancer\" + 0.006*\"lake\" + 0.006*\"central\" + 0.006*\"doubt\" + 0.006*\"illeg\" + 0.005*\"monitor\" + 0.005*\"confirm\"\n",
      "\n",
      "Score: 0.17114600539207458\t \n",
      "Topic: 0.021*\"charg\" + 0.020*\"court\" + 0.016*\"murder\" + 0.013*\"face\" + 0.013*\"jail\" + 0.013*\"blaze\" + 0.012*\"polic\" + 0.011*\"assault\" + 0.011*\"firefight\" + 0.010*\"accus\"\n",
      "\n",
      "Score: 0.012507629580795765\t \n",
      "Topic: 0.011*\"govt\" + 0.010*\"health\" + 0.009*\"servic\" + 0.008*\"opposit\" + 0.008*\"indigen\" + 0.007*\"urg\" + 0.007*\"plan\" + 0.007*\"hospit\" + 0.006*\"union\" + 0.006*\"fund\"\n",
      "\n",
      "Score: 0.012506485916674137\t \n",
      "Topic: 0.010*\"hick\" + 0.009*\"price\" + 0.009*\"liber\" + 0.008*\"govt\" + 0.008*\"drink\" + 0.008*\"sale\" + 0.007*\"drive\" + 0.007*\"say\" + 0.007*\"water\" + 0.006*\"iemma\"\n",
      "\n",
      "Score: 0.012505787424743176\t \n",
      "Topic: 0.010*\"market\" + 0.007*\"solomon\" + 0.007*\"eas\" + 0.007*\"hill\" + 0.007*\"takeov\" + 0.006*\"break\" + 0.006*\"rate\" + 0.006*\"rat\" + 0.006*\"council\" + 0.006*\"rise\"\n",
      "\n",
      "Score: 0.012504915706813335\t \n",
      "Topic: 0.008*\"grower\" + 0.008*\"toll\" + 0.008*\"export\" + 0.007*\"miner\" + 0.006*\"week\" + 0.006*\"coal\" + 0.006*\"theft\" + 0.006*\"afghanistan\" + 0.006*\"revamp\" + 0.006*\"violenc\"\n",
      "\n",
      "Score: 0.012504768557846546\t \n",
      "Topic: 0.010*\"timor\" + 0.010*\"guilti\" + 0.009*\"nuclear\" + 0.009*\"east\" + 0.007*\"iran\" + 0.007*\"plead\" + 0.006*\"uranium\" + 0.005*\"season\" + 0.005*\"agre\" + 0.005*\"injuri\"\n",
      "\n",
      "Score: 0.012504340149462223\t \n",
      "Topic: 0.010*\"councillor\" + 0.009*\"indonesia\" + 0.008*\"speed\" + 0.008*\"steal\" + 0.008*\"retir\" + 0.007*\"chase\" + 0.007*\"polic\" + 0.007*\"polit\" + 0.006*\"australia\" + 0.006*\"england\"\n",
      "\n",
      "Score: 0.012503637000918388\t \n",
      "Topic: 0.023*\"crash\" + 0.021*\"kill\" + 0.016*\"polic\" + 0.014*\"miss\" + 0.012*\"search\" + 0.011*\"die\" + 0.010*\"iraq\" + 0.010*\"investig\" + 0.009*\"attack\" + 0.009*\"dead\"\n"
     ]
    }
   ],
   "source": [
    "#Check which topic our test document belongs to using the LDA TF-IDF model.\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40449c31",
   "metadata": {},
   "source": [
    "#### It has the highest probability (59%) to be part of the topic that we assigned as topic X.\n",
    "\n",
    "---\n",
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c55153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4201062023639679\t Topic: 0.023*\"open\" + 0.019*\"win\" + 0.015*\"time\" + 0.015*\"leav\" + 0.012*\"australian\"\n",
      "Score: 0.2205706089735031\t Topic: 0.040*\"govt\" + 0.019*\"hospit\" + 0.019*\"reject\" + 0.018*\"court\" + 0.018*\"face\"\n",
      "Score: 0.21926896274089813\t Topic: 0.015*\"south\" + 0.015*\"rain\" + 0.014*\"australia\" + 0.012*\"guilti\" + 0.012*\"storm\"\n",
      "Score: 0.02001144550740719\t Topic: 0.024*\"urg\" + 0.021*\"health\" + 0.019*\"servic\" + 0.018*\"worker\" + 0.017*\"govt\"\n",
      "Score: 0.020009590312838554\t Topic: 0.046*\"plan\" + 0.034*\"council\" + 0.019*\"govt\" + 0.019*\"water\" + 0.015*\"group\"\n",
      "Score: 0.020006636157631874\t Topic: 0.032*\"report\" + 0.026*\"iraq\" + 0.021*\"forc\" + 0.017*\"kill\" + 0.016*\"test\"\n",
      "Score: 0.020006630569696426\t Topic: 0.036*\"charg\" + 0.031*\"polic\" + 0.022*\"miss\" + 0.019*\"murder\" + 0.017*\"attack\"\n",
      "Score: 0.020006630569696426\t Topic: 0.062*\"polic\" + 0.036*\"crash\" + 0.032*\"death\" + 0.026*\"investig\" + 0.020*\"road\"\n",
      "Score: 0.020006630569696426\t Topic: 0.040*\"warn\" + 0.018*\"die\" + 0.017*\"deal\" + 0.017*\"coast\" + 0.016*\"north\"\n",
      "Score: 0.020006630569696426\t Topic: 0.024*\"hous\" + 0.017*\"high\" + 0.016*\"market\" + 0.014*\"busi\" + 0.014*\"record\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"My favorite sports activities are running and swimming.\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83730f54",
   "metadata": {},
   "source": [
    "The model correctly classifies the unseen document with '42'% probability to the X category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b3c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
